{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Notebook](https://github.com/jfozard/community-events/blob/main/jax-controlnet-sprint/dataset_tools/dog_pose_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animal pose dataset preparation for ControlNet training with OpenPifPaf and Hugging Face\n",
    "\n",
    "By: MediaPipe team (Google), Diffusers team (Hugging Face), John Fozard (jfozard)\n",
    "This notebook provides an end-to-end example of preparing a dataset that is compatible with ControlNet training using OpenPifPaf and Hugging Face. Specifically, in this notebook, we\n",
    "\n",
    "Load the Stanford Dogs using TensorFlow Datasets.\n",
    "Prepare conditioning images from the original images of the dataset using OpenPifPaf which has an animal pose model\n",
    "Generate captions of the original images using BLIP-2 using ðŸ¤— Transformers.\n",
    "Prepare the final dataset using ðŸ¤— Datasets and push it to the Hugging Face Hub.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate openpifpaf matplotlib opencv-python-headless tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "## Need to stop tf allocating all of the GPU memory - probably shouldn't mix pt and tf like this.\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "dataset = tfds.load(\"stanford_dogs\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "\n",
    "def crop_center(image):\n",
    "    print(image.shape)\n",
    "    h, w = image.shape[-3], image.shape[-2]\n",
    "    if h > w:\n",
    "        cropped_image = tf.image.crop_to_bounding_box(image, (h - w) // 2, 0, w, w)\n",
    "    else:\n",
    "        cropped_image = tf.image.crop_to_bounding_box(image, 0, (w - h) // 2, h, h)\n",
    "    return tf.image.resize_images(cropped_image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "def preprocess_image(example):\n",
    "    # extract the image and label from the example\n",
    "    image = example['image']\n",
    "    label = example['label']\n",
    "    \n",
    "    shape = tf.shape(image)\n",
    "    height = shape[0]\n",
    "    width = shape[1]\n",
    "    \n",
    "    crop_size = tf.minimum(height, width)\n",
    "    crop_offset_height = (height - crop_size) // 2\n",
    "    crop_offset_width = (width - crop_size) // 2\n",
    "    image = tf.image.crop_to_bounding_box(image, crop_offset_height, crop_offset_width, crop_size, crop_size)\n",
    "\n",
    "    # resize the image to the desired size\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    \n",
    "\n",
    "    # normalize the image\n",
    "    image = tf.cast(image, tf.uint8)\n",
    "\n",
    "    return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, sample in enumerate(dataset.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(sample[\"image\"])\n",
    "    plt.title(f'{sample[\"label\"]} {sample[\"image\"].shape}')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here everything is a picture of a dog, and all dogs are good dogs, so no need to filter the dataset. If some non-dogs had snuck into the dataset these would need to be filtered out. (Note that there are some images that look suspiciously like foxes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB OpenPifPaf has some profiling code included that increases its memory usage - these are in the decoder.py file. We may need to patch this properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpifpaf\n",
    "openpifpaf.show.Canvas.show = True\n",
    "openpifpaf.show.Canvas.image_min_dpi = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = openpifpaf.Predictor(checkpoint='shufflenetv2k30-animalpose')\n",
    "predictor.long_edge = 512\n",
    "predictor.preprocess = predictor._preprocess_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = openpifpaf.plugins.animalpose.animal_kp.AnimalKp().head_metas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the built-in annotation code to plot the reference skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE CODE\n",
    "# first make an annotation\n",
    "ann_animal = openpifpaf.Annotation.from_cif_meta(meta)\n",
    "\n",
    "# visualize the annotation\n",
    "openpifpaf.show.KeypointPainter.show_joint_scales = False\n",
    "openpifpaf.show.KeypointPainter.line_width = 3\n",
    "keypoint_painter = openpifpaf.show.KeypointPainter()\n",
    "with openpifpaf.show.Canvas.annotation(ann_animal) as ax:\n",
    "    keypoint_painter.annotation(ax, ann_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different keypoints for dogs compared to humans - we list these here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the example code from the openpose website to detect the pose keypoints on our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataset))\n",
    "im = np.asarray(sample['image'])\n",
    "print(im.shape)\n",
    "predictions, gt_anns, image_meta = predictor.numpy_image(im)\n",
    "annotation_painter = openpifpaf.show.AnnotationPainter()\n",
    "with openpifpaf.show.image_canvas(im) as ax:\n",
    "    annotation_painter.annotations(ax, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenPifaf predictions are part of this prediction structure. I think the columns of the array are x and y position, and the score of that keypoint. The standard plotting functions in Openpifaf draw all keypoints > 0.0, and draw solid lines if the keypoints at either ends have scores > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openpifaf uses the following colour scheme\n",
    "# color = matplotlib.cm.get_cmap('tab20')((color % 20 + 0.05) / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpifaf_colors = [matplotlib.cm.get_cmap('tab20')((i % 20 + 0.05) / 20) for i in range(20)]\n",
    "openpifaf_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably better for ControlNet to use fully saturated colours - we want them to be as different as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connections listed in the OpenPifaf metadata are for a 1-based array but in python we prefer zero-based, so subtract 1 from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_based_skeleton = [ (a-1, b-1) for a,b in meta.draw_skeleton]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted MediaPipe Pose annotation for ControlNet.\n",
    "# There are fewer keypoints (20) and different connections\n",
    "# for animals compared to humans\n",
    "\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "# Body joint color map. #BGR\n",
    "_JOINT_CMAP = {\n",
    "    0: [0, 0, 255],\n",
    "    1: [255, 208, 0],\n",
    "    2: [255, 161, 0],\n",
    "    3: [255, 114, 0],\n",
    "    4: [0, 189, 255],\n",
    "    5: [0, 236, 255],\n",
    "    6: [0, 255, 226],\n",
    "    7: [255, 0, 76],\n",
    "    8: [0, 255, 131],\n",
    "    9: [255, 0, 171],\n",
    "    10: [0, 255, 37],\n",
    "    11: [244, 0, 253],\n",
    "    12: [57, 255, 0],\n",
    "    13: [151, 0, 255],\n",
    "    14: [151, 255, 0],\n",
    "    15: [57, 0, 255],\n",
    "    16: [245, 255, 0],\n",
    "    17: [0, 39, 255],\n",
    "    18: [255, 169, 0],\n",
    "    19: [0, 133, 255],\n",
    "}\n",
    "\n",
    "\n",
    "# Connection color map. #BGR\n",
    "# These colors probably matter a little, and it's not clear\n",
    "# exactly how to choose them\n",
    "# For instance - should they be similar to the keypoint colours or not?\n",
    "# I've just taken them from the example notebook\n",
    "\n",
    "_CONNECTION_CMAP = {\n",
    " (0, 1): [127, 104, 127],\n",
    " (0, 2): [0, 94, 255],\n",
    " (0, 5): [255, 184, 0],\n",
    " (1, 3): [255, 137, 0],\n",
    " (2, 4): [255, 57, 38],\n",
    " (1, 2): [0, 212, 255],\n",
    " (5, 7): [0, 245, 240],\n",
    " (5, 8): [0, 255, 178],\n",
    " (5, 9): [127, 127, 104],\n",
    " (6, 7): [150, 127, 126],\n",
    " (6, 10): [197, 0, 254],\n",
    " (6, 11): [122, 127, 221],\n",
    " (9, 13): [104, 255, 0],\n",
    " (13, 17): [156, 127, 56],\n",
    " (8, 12): [104, 0, 255],\n",
    " (12, 16): [198, 255, 0],\n",
    " (11, 15): [28, 19, 255],\n",
    " (15, 19): [28, 66, 255],\n",
    " (10, 14): [28, 114, 255],\n",
    " (14, 18): [250, 212, 0],\n",
    "}\n",
    "\n",
    "# The connections liste\n",
    "In [19]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pose(\n",
    "    image: np.ndarray,\n",
    "    landmark_list: Any,\n",
    "    connections: Any,\n",
    "    overlay: bool = True,\n",
    ") -> tuple[np.ndarray, dict[str, list[float]]]:\n",
    "    \"\"\"Draws the landmarks and the connections on the image.\n",
    "\n",
    "    Args:\n",
    "      image: A three channel BGR image represented as numpy ndarray.\n",
    "      landmark_list: A normalized landmark list proto message to be annotated on\n",
    "        the image.\n",
    "      connections: A list of landmark index tuples that specifies how landmarks to\n",
    "        be connected in the drawing.\n",
    "      overlay: Whether to overlay on the input image.\n",
    "\n",
    "    Returns:\n",
    "      (image, dictionary).\n",
    "\n",
    "    Raises:\n",
    "      ValueError: If one of the following:\n",
    "        a) If the input image is not three channel BGR.\n",
    "        b) If any connetions contain invalid landmark index.\n",
    "    \"\"\"\n",
    "    if image.shape[2] != 3:\n",
    "        raise ValueError(\"Input image must contain three channel bgr data.\")\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    min_length = min(image_rows, image_cols)\n",
    "    draw_line_width = math.floor(min_length * 0.01)\n",
    "    draw_circle_radius = math.floor(min_length * 0.015)\n",
    "    idx_to_coordinates = {}\n",
    "    \n",
    "    for idx, landmark in enumerate(landmark_list):\n",
    "        if landmark[2]>0.0:\n",
    "            idx_to_coordinates[idx] = (int(landmark[0]), int(landmark[1]))\n",
    "            \n",
    "    if overlay:\n",
    "        output_image = image.copy()\n",
    "    else:\n",
    "        output_image = np.zeros(\n",
    "            list(image.shape[:2])\n",
    "            + [\n",
    "                3,\n",
    "            ],\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    if connections:\n",
    "        num_landmarks = len(landmark_list)\n",
    "        # Draws the connections if the start and end landmarks are both visible.\n",
    "        for connection in connections:\n",
    "            start_idx = connection[0]\n",
    "            end_idx = connection[1]\n",
    "            if not (0 <= start_idx < num_landmarks and 0 <= end_idx < num_landmarks):\n",
    "                raise ValueError(\n",
    "                    \"Landmark index is out of range. Invalid connection \"\n",
    "                    f\"from landmark #{start_idx} to landmark #{end_idx}.\"\n",
    "                )\n",
    "            if start_idx in idx_to_coordinates and end_idx in idx_to_coordinates:\n",
    "                cv2.line(\n",
    "                    output_image,\n",
    "                    pt1=idx_to_coordinates[start_idx],\n",
    "                    pt2=idx_to_coordinates[end_idx],\n",
    "                    color=_CONNECTION_CMAP[(start_idx, end_idx)],\n",
    "                    thickness=draw_line_width,\n",
    "                )\n",
    "\n",
    "    # Draws landmark points after finishing the connection lines, which is\n",
    "    # aesthetically better.\n",
    "    for idx, landmark_px in idx_to_coordinates.items():\n",
    "        # Fill color into the circle\n",
    "        cv2.circle(\n",
    "            output_image,\n",
    "            center=landmark_px,\n",
    "            radius=draw_circle_radius,\n",
    "            color=_JOINT_CMAP[idx],\n",
    "            thickness=-1,\n",
    "        )\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def determine_pose(image):\n",
    "    \"\"\"Estimates pose and creates an image with just the pose body points.\n",
    "\n",
    "    The image consisting the pose body points serves as the conditioning image\n",
    "    for ControlNet training.\n",
    "\n",
    "    Args:\n",
    "        image: A three channel RGB image represented as a tf.Tensor.\n",
    "\n",
    "    Returns:\n",
    "        A tuple consisting of the original image (`image`), an image where\n",
    "        the original image is overlaid with the pose keypoints, and an image\n",
    "        with just the pose keypoints.\n",
    "    \"\"\"\n",
    "    image = np.asarray(image)\n",
    "    \n",
    "    predictions, gt_anns, image_meta = predictor.numpy_image(image)\n",
    "    if not predictions:\n",
    "        data = np.zeros((20,3))\n",
    "    else:\n",
    "        data = predictions[0].data\n",
    "\n",
    "    # Draw pose landmarks on a copy of the input image.\n",
    "    annotated_image = draw_pose(\n",
    "        image, data, zero_based_skeleton\n",
    "    )\n",
    "\n",
    "        # Draw pose landmarks on a blank image.\n",
    "    blank_image = draw_pose(\n",
    "        image, data, zero_based_skeleton, False\n",
    "    )\n",
    "\n",
    "    return image, annotated_image, blank_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test determine_pose on a single image\n",
    "it = iter(dataset)\n",
    "sample = next(it)\n",
    "output_image, annotated_image, black_image = determine_pose(sample['image'])\n",
    "plt.imshow(annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_vis = 10\n",
    "fig, ax = plt.subplots(ncols=3, nrows=num_samples_to_vis, figsize=(5, 15))\n",
    "for i, sample in enumerate(dataset.take(num_samples_to_vis).as_numpy_iterator()):\n",
    "    \n",
    "    \n",
    "    sample = determine_pose(np.array(sample['image']))\n",
    "    original_image = sample[0]\n",
    "    annotated_image, blank_image = sample[1], sample[2]\n",
    "\n",
    "    \n",
    "    \n",
    "    samples = [original_image, annotated_image, blank_image]\n",
    "    titles = [\"Input\", \"Annotation Overlaid\", \"Annotation\"]\n",
    "\n",
    "    for j in range(3):\n",
    "        ax[i, j].imshow(samples[j])\n",
    "        ax[i, j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption generation with BLIP-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we leverage an open-source and powerful pre-trained model for caption generation - BLIP-2. This model comes with different checkpoints, among which we're using Salesforce/blip2-flan-t5-xl.\n",
    "\n",
    "ðŸ¤— Transformers also supports its predecessor - BLIP.\n",
    "\n",
    "In the cell below, we first load the following classes: Blip2Processor and Blip2ForConditionalGeneration. Then we write a utility to generate captions for a given batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('torch device', device)\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "captioning_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16\n",
    ")\n",
    "captioning_model = captioning_model.to(device)\n",
    "\n",
    "\n",
    "def generate_captions(images: np.ndarray) -> list[str]:\n",
    "    \"\"\"Generates captions for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        images: A batch of images in the RGB format.\n",
    "\n",
    "    Returns:\n",
    "        A list of generated captions.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=images, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "    generated_ids = captioning_model.generate(**inputs)\n",
    "    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    generated_texts = [text.strip() for text in generated_texts]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we serialize the original images, their corresponding conditioning images, and generate BLIP captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "batch_size = 1\n",
    "data_root = \"data\"\n",
    "#new_dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_image(\n",
    "    original_image: np.ndarray,\n",
    "    overlaid_annotation: np.ndarray,\n",
    "    blank_annotation: np.ndarray,\n",
    "):\n",
    "    \"\"\"Serializes images to `data_root`.\"\"\"\n",
    "    image_hash = hashlib.sha1(original_image.tobytes()).hexdigest()\n",
    "    PIL.Image.fromarray(original_image).save(\n",
    "        os.path.join(data_root, f\"{str(image_hash)}_original.png\")\n",
    "    )\n",
    "    PIL.Image.fromarray(overlaid_annotation).save(\n",
    "        os.path.join(data_root, f\"{str(image_hash)}_overlaid.png\")\n",
    "    )\n",
    "    PIL.Image.fromarray(blank_annotation).save(\n",
    "        os.path.join(data_root, f\"{str(image_hash)}_condition.png\")\n",
    "    )\n",
    "    return image_hash\n",
    "\n",
    "\n",
    "original_image_paths = []\n",
    "overlaid_annotation_paths = []\n",
    "blank_annotation_paths = []\n",
    "all_generated_captions = []\n",
    "total = 0\n",
    "\n",
    "for samples in tqdm.tqdm(dataset.as_numpy_iterator()):\n",
    "    samples = determine_pose(samples['image'])\n",
    "    original_images = samples[0]\n",
    "    overlaid_annotations = samples[1]\n",
    "    blank_annotations = samples[2]\n",
    "\n",
    "    generated_captions = generate_captions(original_images)\n",
    "\n",
    "\n",
    "    image_hash = save_image(\n",
    "        original_images, overlaid_annotations, blank_annotations\n",
    "    )\n",
    "    original_image_paths.append(\n",
    "        os.path.join(data_root, f\"{str(image_hash)}_original.png\")\n",
    "    )\n",
    "    overlaid_annotation_paths.append(\n",
    "        os.path.join(data_root, f\"{str(image_hash)}_overlaid.png\")\n",
    "    )\n",
    "    blank_annotation_paths.append(\n",
    "        os.path.join(data_root, f\"{str(image_hash)}_condition.png\")\n",
    "    )\n",
    "\n",
    "    all_generated_captions.append(generated_captions)\n",
    "\n",
    "    total += 1\n",
    "\n",
    "print(f\"{total} samples will go in the final dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ðŸ¤— Dataset and push to the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we leverage ðŸ¤— Datasets to create a the final dataset that is ready to go with our ControlNet training script.\n",
    "\n",
    "Finally, we push the prepared dataset to Hub for easy sharing with the community. To be able to do that, you need to be a registered Hugging Face user to authenticate yourself. If you are not one already, please head over hf.co and register youself; it's free =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a generator that yields a tuple consisting of:\n",
    "\n",
    "original_image\n",
    "condtioning_image\n",
    "overlaid\n",
    "caption\n",
    "Note that overlaid image is just there for convenience. It is not mandatory for us to include it while creating the dataset as it won't be used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "\n",
    "def gen_examples():\n",
    "    for i in range(len(original_image_paths)):\n",
    "        im = imread(blank_annotation_paths[i])\n",
    "        if not im.sum():\n",
    "            continue\n",
    "        else:\n",
    "            yield {\n",
    "                \"original_image\": {\"path\": original_image_paths[i]},\n",
    "                \"conditioning_image\": {\"path\": blank_annotation_paths[i]},\n",
    "                \"overlaid\": {\"path\": overlaid_annotation_paths[i]},\n",
    "                \"caption\": all_generated_captions[i],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features\n",
    "from datasets import Image as ImageFeature\n",
    "from datasets import Value\n",
    "\n",
    "final_dataset = Dataset.from_generator(\n",
    "    gen_examples,\n",
    "    features=Features(\n",
    "        original_image=ImageFeature(),\n",
    "        conditioning_image=ImageFeature(),\n",
    "        overlaid=ImageFeature(),\n",
    "        caption=Value(\"string\"),\n",
    "    ),\n",
    "    num_proc=6,\n",
    ")\n",
    "\n",
    "ds_name = \"dog-poses-controlnet-dataset\"\n",
    "final_dataset.push_to_hub(ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_image_paths), len(blank_annotation_paths), len(overlaid_annotation_paths), len(all_generated_captions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
